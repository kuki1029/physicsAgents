from physicsAgents.application.conversation.workflow.graph import initiate_workflow
from typing import AsyncGenerator, Any
from langchain_core.messages import AIMessageChunk, HumanMessage, AIMessage
import uuid
from langgraph.checkpoint.memory import InMemorySaver


checkpointer = InMemorySaver()


async def get_ws_chat_res(
    messages: str | list[str] | list[dict[str, Any]],
    phys_id: str,
    phys_name: str,
    phys_style: str,
    user_id: str = "",
) -> AsyncGenerator[str, None]:
    """Sends conversation details through workflow graph and gets streaming response

    Args:
        messages: Initial message or thread of messages (with metadata) for the conversation
        phys_id: ID of the physicist
        phys_name: Name of the physicist
        phys_style: Talking style of the physicist
        new_thread: Whether to start a new conversation or not for a new user

    Returns:
        tuple[str, PhysicistState]: Tuple contains content of most recent msg and state of workflow
    """
    graph = initiate_workflow().compile(checkpointer=checkpointer)

    try:
        thread_id = f"{user_id}-{phys_id}"

        config = {"configurable": {"thread_id": thread_id}}
        async for text in graph.astream(
            input={
                "messages": messages,
                "physicist_name": phys_name,
                "physicist_style": phys_style,
            },
            config=config,
            stream_mode="messages",
        ):
            # TODO use types for conversation as its confusing
            if text[1]["langgraph_node"] == "conversation" and isinstance(
                text[0], AIMessageChunk
            ):
                yield text[0].content

    except Exception as e:
        print(e)
        raise RuntimeError(f"Cannot stream conversation flow: {str(e)}") from e


async def get_chat_response(messages: str) -> str:
    """
        Asynchronously generates a chat response from a compiled workflow graph.

    This function initializes and compiles a workflow graph, then invokes it
    asynchronously with the provided input message string. It retrieves the
    last message from the output and returns its content as the chat response.

    Args:
        messages (str): A string representing the user input or conversation history
                        to be processed by the workflow.

    Returns:
        str: The content of the final message generated by the workflow.

    Raises:
        RuntimeError: If the workflow invocation fails or any error occurs during execution.
    """
    graph = initiate_workflow().compile()

    try:
        output = await graph.ainvoke(input={"messages": messages})
        msg = output["messages"][-1]
        return msg.content
    except Exception as e:
        raise RuntimeError(f"Could not get response: {str(e)}") from e


# def __format_messages(
#     msgs: str | list[dict[str, Any]],
# ) -> list[HumanMessage | AIMessage]:
#     """Convert different formats of msgs to Langchain msg objects

#     Args:
#         msgs: Can be str, list of str, dict with role, content keys

#     Returns:
#         List[HumanMessage | AIMessage]: List of langchain msg objects
#     """
#     if isinstance(msgs, str):
#         return [HumanMessage(content=msgs)]

#     if isinstance(msgs, list)
